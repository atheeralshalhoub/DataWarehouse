# DataWarehouse

# Song Play Project .. 

##### Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

##### As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

### Firstly : recognize the dataset ..
##### - The first dataset is a subset of real data from the Million Song Dataset.
##### - The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.


#### both of them wrriten as JSON format ..

### Secondly : Schema for Song Play Analysis ..
#### - **Fact Table** :
**songplays** - records in log data associated with song plays i.e. records with page NextSong.
* songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### - **Dimension Tables** : 
**users** - users in the app .
* user_id, first_name, last_name, gender, level
**songs** - songs in music database .
* song_id, title, artist_id, year, duration
**artists** - artists in music database .
* artist_id, name, location, latitude, longitude
**time** - timestamps of records in songplays broken down into specific units .
* start_time, hour, day, week, month, year, weekday

### Thirdly : Project Steps ..
#### Open sql_queries.py 
* Write CREATE statements to create each table.
* Check Every datatype in our dataset that suitable.
* Write DROP statements to drop each table if it exists.
* Write INSERT statements to insert values directly by calling query.

#### Open Terminal ..
* Run create_tables.py to create your database and tables.



#### open IaC Note Book ..
* imports pandas, boto3, json, psycopg2

#### open AWS services in browser ..
- Create a new IAM user (atheeralshalhoub) in your AWS account
- Give it `AdministratorAccess`, From `Attach existing policies directly` Tab
- Take note of the access key and secret 

#### open dhw.cfg ..
* write key, secret that related to IAM user in dwh.cfg
* fill the values in other values.

#### open IaC Note Book ..
# Load DWH Params from a file
- import configparser
  config = configparser.ConfigParser()
  config.read_file(open('dwh.cfg'))

  KEY                    = config.get('AWS','KEY')
  SECRET                 = config.get('AWS','SECRET')

  DWH_CLUSTER_TYPE       = config.get("DWH","DWH_CLUSTER_TYPE")
  DWH_NUM_NODES          = config.get("DWH","DWH_NUM_NODES")
  DWH_NODE_TYPE          = config.get("DWH","DWH_NODE_TYPE")

  DWH_CLUSTER_IDENTIFIER = config.get("DWH","DWH_CLUSTER_IDENTIFIER")
  DWH_DB                 = config.get("DWH","DWH_DB")
  DWH_DB_USER            = config.get("DWH","DWH_DB_USER")
  DWH_DB_PASSWORD        = config.get("DWH","DWH_DB_PASSWORD")
  DWH_PORT               = config.get("DWH","DWH_PORT")

  DWH_IAM_ROLE_NAME      = config.get("DWH", "DWH_IAM_ROLE_NAME")

  (DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)

   pd.DataFrame({"Param": ["DWH_CLUSTER_TYPE", "DWH_NUM_NODES", "DWH_NODE_TYPE",
   "DWH_CLUSTER_IDENTIFIER","DWH_DB","DWH_DB_USER", "DWH_DB_PASSWORD", "DWH_PORT", "DWH_IAM_ROLE_NAME"],
    "Value": [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER,
    DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]
             })

**output***


# Create clients for IAM, EC2, S3 and Redshift
- import boto3

  ec2 = boto3.resource('ec2',
                       region_name="us-west-2",
                       aws_access_key_id=KEY,
                       aws_secret_access_key=SECRET
                    )

  s3 = boto3.resource('s3',
                       region_name="us-west-2",
                       aws_access_key_id=KEY,
                       aws_secret_access_key=SECRET
                   )

  iam = boto3.client('iam',aws_access_key_id=KEY,
                     aws_secret_access_key=SECRET,
                     region_name='us-west-2'
                  )

  redshift = boto3.client('redshift',
                       region_name="us-west-2",
                       aws_access_key_id=KEY,
                       aws_secret_access_key=SECRET
                       )
# Check out the sample data sources on S3
- sampleDbBucket =  s3.Bucket("awssampledbuswest2")
  for obj in sampleDbBucket.objects.filter(Prefix="ssbgz"):
    print(obj)
**output***


# STEP 1: IAM ROLE
- Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)
- from botocore.exceptions import ClientError

  #1.1 Create the role, 
  try:
    print("1.1 Creating a new IAM Role") 
    dwhRole = iam.create_role(
        Path='/',
        RoleName=DWH_IAM_ROLE_NAME,
        Description = "Allows Redshift clusters to call AWS services on your behalf.",
        AssumeRolePolicyDocument=json.dumps(
            {'Statement': [{'Action': 'sts:AssumeRole',
               'Effect': 'Allow',
               'Principal': {'Service': 'redshift.amazonaws.com'}}],
             'Version': '2012-10-17'})
    )    
  except Exception as e:
    print(e)
    
    
  print("1.2 Attaching Policy")

  iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,
                       PolicyArn="arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
                      )['ResponseMetadata']['HTTPStatusCode']

  print("1.3 Get the IAM role ARN")
  roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']

  print(roleArn)

**output***


# STEP 2:  Redshift Cluster
- Create a RedShift Cluster
- For complete arguments to `create_cluster`
- try:
    response = redshift.create_cluster(        
       #HW
    ClusterType=DWH_CLUSTER_TYPE,
    NodeType=DWH_NODE_TYPE,
    NumberOfNodes=int(DWH_NUM_NODES),

      #Identifiers & Credentials
    DBName=DWH_DB,
    ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,
    MasterUsername=DWH_DB_USER,
    MasterUserPassword=DWH_DB_PASSWORD,
        
        #Roles (for s3 access)
    IamRoles=[roleArn]  
    )
  except Exception as e:
    print(e)
    

## 2.1 *Describe* the cluster to see its status
- run this block several times until the cluster status becomes `Available`
- def prettyRedshiftProps(props):
    pd.set_option('display.max_colwidth', -1)
    keysToShow = ["ClusterIdentifier", "NodeType", "ClusterStatus", "MasterUsername", "DBName", "Endpoint", 
    "NumberOfNodes",'VpcId']
    x = [(k, v) for k,v in props.items() if k in keysToShow]
    return pd.DataFrame(data=x, columns=["Key", "Value"])

   myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]
   prettyRedshiftProps(myClusterProps)

**output***

<font color='red'>DO NOT RUN THIS unless the cluster status becomes "Available" </font>

- DWH_ENDPOINT = myClusterProps['Endpoint']['Address']
  DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']
  print("DWH_ENDPOINT :: ", DWH_ENDPOINT)
  print("DWH_ROLE_ARN :: ", DWH_ROLE_ARN)
  
**output***

# fill those values in the .cfg file
- config.set('CLUSTER', 'HOST', DWH_ENDPOINT)
  config.set('IAM_ROLE', 'ARN', DWH_ROLE_ARN)
  with open('dwh.cfg', 'w') as f:
  config.write(f)
  
# check those values filled in your `dwh.cfg` file
- [CLUSTER]
  HOST=   DWH_ENDPOINT
  DB_NAME=dwh
  DB_USER=dwhuser
  DB_PASSWORD=Passw0rd
  DB_PORT=5439

- [IAM_ROLE]
  ARN=  DWH_ROLE_ARN

- [S3]
  LOG_DATA='s3://udacity-dend/log_data'
  LOG_JSONPATH='s3://udacity-dend/log_json_path.json'
  SONG_DATA='s3://udacity-dend/song_data'

- [AWS]
  KEY=
  SECRET=

- [DWH] 
  DWH_CLUSTER_TYPE       =multi-node
  DWH_NUM_NODES          =4
  DWH_NODE_TYPE          =dc2.large
  DWH_IAM_ROLE_NAME      =dwhRole
  DWH_CLUSTER_IDENTIFIER =dwhCluster
  DWH_DB                 =dwh
  DWH_DB_USER            =dwhuser
  DWH_DB_PASSWORD        =Passw0rd
  DWH_PORT               =5439

# write host and arn values filled in your `dwh.cfg` file
- config.read_file(open('dwh.cfg'))
- check if the values filed in dwh file 


## STEP 3: Open an incoming  TCP port to access the cluster ednpoint
- try:
    vpc = ec2.Vpc(id=myClusterProps['VpcId'])
    defaultSg = list(vpc.security_groups.all())[0]
    print(defaultSg)
    defaultSg.authorize_ingress(
        GroupName=defaultSg.group_name,
        CidrIp='0.0.0.0/0',
        IpProtocol='TCP',
        FromPort=int(DWH_PORT),
        ToPort=int(DWH_PORT)
    )
  except Exception as e:
    print(e)

**output***

# STEP 4: Make sure you can connect to the cluster
- run "host={} dbname={} user={} password={} port={}".format(*config['CLUSTER'].values())

**output***

## connect with db 
- conn = psycopg2.connect("host={} dbname={} user={} password={} port={}".format(*config['CLUSTER'].values()))
  cur = conn.cursor()

  print('Connected')

  conn.close()

**output***


#### open AWS services ( Amazon Redshift ) in browser ..
- open query editor
- test all queries in sql_queries.

#### Lastly : Open Terminal ..
* Run create_tables.py to create your database and tables.
* Run etl.py

#### open AWS services ( Amazon Redshift ) in browser ..
- open query editor
- run select statments for artist_events table
  SELECT * from [public].staging_events;


- run select statments for staging_songs table
  SELECT * from [public].staging_songs;


- run select statments for songplays table
  SELECT * from [public].songplays;


- run select statments for users table
  SELECT * from [public].users;


- run select statments for songs table
  SELECT * from [public].songs;


- run select statments for artists table
  SELECT * from [public].artists;


- run select statments for time table
  SELECT * from [public].time;



#### open IaC Note Book ..
# STEP 5: Clean up your resources
- Delete cluster
  redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)
- check cluster status
  myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]
  prettyRedshiftProps(myClusterProps)
- delete role
  iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn="arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess")
  iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)
